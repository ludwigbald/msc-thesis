@article{abdar2021ReviewUncertaintyQuantification,
  title = {A Review of Uncertainty Quantification in Deep Learning: {{Techniques}}, Applications and Challenges},
  shorttitle = {A Review of Uncertainty Quantification in Deep Learning},
  author = {Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and Fieguth, Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya, U. Rajendra and Makarenkov, Vladimir and Nahavandi, Saeid},
  year = {2021},
  month = dec,
  journal = {Information Fusion},
  volume = {76},
  pages = {243--297},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2021.05.008},
  abstract = {Uncertainty quantification (UQ) methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes. They have been applied to solve a variety of real-world problems in science and engineering. Bayesian approximation and ensemble learning techniques are two widely-used types of uncertainty quantification (UQ) methods. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlights fundamental research challenges and directions associated with UQ.},
  langid = {english},
  keywords = {Artificial intelligence,Bayesian statistics,Deep learning,Ensemble learning,Machine learning,Uncertainty quantification},
  file = {/Users/ludwig/Zotero/storage/FX3LALJN/Abdar et al. - 2021 - A review of uncertainty quantification in deep lea.pdf;/Users/ludwig/Zotero/storage/FNVMA5TD/S1566253521001081.html}
}

@book{AgentbasedModelsEconomy,
  title = {Agent-Based {{Models}} of the {{Economy}}},
  langid = {english},
  file = {/Users/ludwig/Zotero/storage/9634RUSW/Agent-based Models of the Economy.pdf;/Users/ludwig/Zotero/storage/FG8K4V2Y/9781137339812.html}
}

@inproceedings{azizzadenesheli2018EfficientExplorationBayesian,
  title = {Efficient {{Exploration Through Bayesian Deep Q-Networks}}},
  booktitle = {2018 {{Information Theory}} and {{Applications Workshop}} ({{ITA}})},
  author = {Azizzadenesheli, Kamyar and Brunskill, Emma and Anandkumar, Animashree},
  year = {2018},
  month = feb,
  pages = {1--9},
  doi = {10.1109/ITA.2018.8503252},
  abstract = {We propose Bayesian Deep Q-Network (BDQN), a practical Thompson sampling based Reinforcement Learning (RL) Algorithm. Thompson sampling allows for targeted exploration in high dimensions through posterior sampling but is usually computationally expensive. We address this limitation by introducing uncertainty only at the output layer of the network through a Bayesian Linear Regression (BLR) model, which can be trained with fast closed-form updates and its samples can be drawn efficiently through the Gaussian distribution. We apply our method to a wide range of Atari games in Arcade Learning Environments. Since BDQN carries out more efficient exploration, it is able to reach higher rewards substantially faster than a key baseline, double deep Q network DDQN.},
  keywords = {Bayes methods,Complexity theory,Games,Linear regression,Neural networks,Uncertainty},
  file = {/Users/ludwig/Zotero/storage/VTVHU2JW/Azizzadenesheli et al. - 2018 - Efficient Exploration Through Bayesian Deep Q-Netw.pdf;/Users/ludwig/Zotero/storage/HJEW6Y2I/8503252.html}
}

@misc{biagioni2021PowerGridworldFrameworkMultiAgent,
  title = {{{PowerGridworld}}: {{A Framework}} for {{Multi-Agent Reinforcement Learning}} in {{Power Systems}}},
  shorttitle = {{{PowerGridworld}}},
  author = {Biagioni, David and Zhang, Xiangyu and Wald, Dylan and Vaidhynathan, Deepthi and Chintala, Rohit and King, Jennifer and Zamzam, Ahmed S.},
  year = {2021},
  month = nov,
  number = {arXiv:2111.05969},
  eprint = {2111.05969},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2111.05969},
  abstract = {We present the PowerGridworld software package to provide users with a lightweight, modular, and customizable framework for creating power-systems-focused, multi-agent Gym environments that readily integrate with existing training frameworks for reinforcement learning (RL). Although many frameworks exist for training multi-agent RL (MARL) policies, none can rapidly prototype and develop the environments themselves, especially in the context of heterogeneous (composite, multi-device) power systems where power flow solutions are required to define grid-level variables and costs. PowerGridworld is an open-source software package that helps to fill this gap. To highlight PowerGridworld's key features, we present two case studies and demonstrate learning MARL policies using both OpenAI's multi-agent deep deterministic policy gradient (MADDPG) and RLLib's proximal policy optimization (PPO) algorithms. In both cases, at least some subset of agents incorporates elements of the power flow solution at each time step as part of their reward (negative cost) structures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/ludwig/Zotero/storage/KEK4A2WF/Biagioni et al. - 2021 - PowerGridworld A Framework for Multi-Agent Reinfo.pdf;/Users/ludwig/Zotero/storage/K24NALJW/2111.html}
}

@phdthesis{canteli2020MultiagentReinforcementLearning,
  type = {Thesis},
  title = {Multi-Agent Reinforcement Learning for Demand Response and Load Shaping of Grid-Interactive Connected Buildings},
  author = {Canteli, V{\'a}zquez and Ram{\'o}n, Jos{\'e}},
  year = {2020},
  month = sep,
  abstract = {Increasing electrification, integration of renewable energy resources, rapid urbanization, and the potential shift towards higher integration of electric vehicles and other distributed energy resources, such as batteries, represent challenges that the energy sector will have to face in the future. As these changes begin to happen, buildings will have more energy storage and generation capacity, which can create additional uncertainty and volatility, but also more flexibility if these systems are appropriately controlled.  Demand response can enable consumers to reduce their energy consumption through load curtailment, shift their energy consumption over time to periods of higher renewable energy generation, or generate and store energy at certain times to provide the grid with more flexibility. Model-based control approaches, such as model predictive control, can provide near optimal control policies for demand response. However, they require to develop accurate models of the systems to be controlled, which is not always a cost-effective scalable option in complex, unpredictable or non-stationary systems. On the other hand, model-free control algorithms, such as reinforcement learning, have the potential to provide good control policies in a cost-effective manner. In this dissertation, I review the literature on reinforcement learning for demand response and control of urban energy systems, and introduce a novel multi-agent reinforcement learning algorithm specifically designed to be decentralized, scalable, and implemented in a demand response setting. This multi-agent reinforcement learning algorithm outperforms its single-agent counterparts and a baseline rule-based controller by more than 15\% on an average of 5 different metrics: load factor, net electricity consumption, peak demand, average daily peak demand, and ramping. I test the controllers in CityLearn, an Open AI Gym environment I created for the implementation of single and multi-agent reinforcement learning for demand response. CityLearn is also intended to be used by other researchers and tackle another problem I observed in my review of the literature: lack of standardization and reproducibility of most research in RL applied to energy management in urban settings.},
  langid = {english},
  annotation = {Accepted: 2021-10-18T20:48:55Z},
  file = {/Users/ludwig/Zotero/storage/ASHDN3BM/Canteli und Ram√≥n - 2020 - Multi-agent reinforcement learning for demand resp.pdf;/Users/ludwig/Zotero/storage/889EL95W/89240.html}
}

@inproceedings{chen2020LearningDistributedControl,
  title = {Learning a {{Distributed Control Scheme}} for {{Demand Flexibility}} in {{Thermostatically Controlled Loads}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Communications}}, {{Control}}, and {{Computing Technologies}} for {{Smart Grids}} ({{SmartGridComm}})},
  author = {Chen, Bingqing and Yao, Weiran and Francis, Jonathan and Berg{\'e}s, Mario},
  year = {2020},
  month = nov,
  pages = {1--7},
  doi = {10.1109/SmartGridComm47815.2020.9302954},
  abstract = {Demand flexibility is increasingly important for power grids, in light of growing penetration of renewable generation. Careful coordination of thermostatically controlled loads (TCLs) can potentially modulate energy demand, decrease operating costs, and increase grid resiliency. However, it is challenging to control a heterogeneous population of TCLs: the control problem has a large state action space; each TCL has unique and complex dynamics; and multiple system-level objectives need to be optimized simultaneously. To address these challenges, we propose a distributed control solution, which consists of a central load aggregator that optimizes system-level objectives and building-level controllers that track the load profiles planned by the aggregator. To optimize our agents' policies, we draw inspirations from both reinforcement learning (RL) and model predictive control. Specifically, the aggregator is updated with an evolutionary strategy, which was recently demonstrated to be a competitive and scalable alternative to more sophisticated RL algorithms and enables policy updates independent of the building-level controllers. We evaluate our proposed approach across four climate zones in four nine-building clusters, using the newly-introduced CityLearn simulation environment. Our approach achieved an average reduction of 16.8\% in the environment cost compared to the benchmark rule-based controller.},
  keywords = {Aggregates,Buildings,Decentralized control,demand flexibility,evolutionary strategies,Load modeling,Predictive models,reinforcement learning,Sociology,Statistics,thermostatically controlled loads},
  file = {/Users/ludwig/Zotero/storage/BLG9RQEI/Chen et al. - 2020 - Learning a Distributed Control Scheme for Demand F.pdf;/Users/ludwig/Zotero/storage/6AM7QM97/9302954.html}
}

@inproceedings{christiano2017DeepReinforcementLearning,
  title = {Deep {{Reinforcement Learning}} from {{Human Preferences}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. Our approach separates learning the goal from learning the behavior to achieve it. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on about 0.1\% of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.},
  file = {/Users/ludwig/Zotero/storage/NMQS3QQF/Christiano et al. - 2017 - Deep Reinforcement Learning from Human Preferences.pdf}
}

@inproceedings{christiano2017DeepReinforcementLearninga,
  title = {Deep {{Reinforcement Learning}} from {{Human Preferences}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. Our approach separates learning the goal from learning the behavior to achieve it. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on about 0.1\% of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.},
  file = {/Users/ludwig/Zotero/storage/8D4ED7ZL/Christiano et al. - 2017 - Deep Reinforcement Learning from Human Preferences.pdf}
}

@inproceedings{chua2018DeepReinforcementLearning,
  title = {Deep {{Reinforcement Learning}} in a {{Handful}} of {{Trials}} Using {{Probabilistic Dynamics Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g. 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).},
  file = {/Users/ludwig/Zotero/storage/LEPH5ULG/Chua et al. - 2018 - Deep Reinforcement Learning in a Handful of Trials.pdf}
}

@misc{clements2020EstimatingRiskUncertainty,
  title = {Estimating {{Risk}} and {{Uncertainty}} in {{Deep Reinforcement Learning}}},
  author = {Clements, William R. and Van Delft, Bastien and Robaglia, Beno{\^i}t-Marie and Slaoui, Reda Bahi and Toth, S{\'e}bastien},
  year = {2020},
  month = sep,
  number = {arXiv:1905.09638},
  eprint = {1905.09638},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.09638},
  abstract = {Reinforcement learning agents are faced with two types of uncertainty. Epistemic uncertainty stems from limited data and is useful for exploration, whereas aleatoric uncertainty arises from stochastic environments and must be accounted for in risk-sensitive applications. We highlight the challenges involved in simultaneously estimating both of them, and propose a framework for disentangling and estimating these uncertainties on learned Q-values. We derive unbiased estimators of these uncertainties and introduce an uncertainty-aware DQN algorithm, which we show exhibits safe learning behavior and outperforms other DQN variants on the MinAtar testbed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ludwig/Zotero/storage/NJJAMNAX/Clements et al. - 2020 - Estimating Risk and Uncertainty in Deep Reinforcem.pdf;/Users/ludwig/Zotero/storage/DWMDLM2U/1905.html}
}

@inproceedings{cruz2018ActionSelectionMethods,
  title = {Action {{Selection Methods}} in a {{Robotic Reinforcement Learning Scenario}}},
  booktitle = {2018 {{IEEE Latin American Conference}} on {{Computational Intelligence}} ({{LA-CCI}})},
  author = {Cruz, Francisco and W{\"u}ppen, Peter and Fazrie, Alvin and Weber, Cornelius and Wermter, Stefan},
  year = {2018},
  month = nov,
  pages = {1--6},
  doi = {10.1109/LA-CCI.2018.8625243},
  abstract = {Reinforcement learning allows an agent to learn a new task while autonomously exploring its environment. For this aim, the agent chooses an action to perform among the available ones for a certain state. Nonetheless, a common problem for a reinforcement learning agent is to find a proper balance between exploration and exploitation of actions in order to achieve an optimal behavior. This paper compares multiple approaches to the exploration/exploitation dilemma in reinforcement learning and, moreover, it implements an exemplary reinforcement learning task within the domain of domestic robotics to show the performance of different exploration policies on it. We perform the domestic task using {$\epsilon$}-greedy, softmax, VDBE, and VDBE-Softmax with online and offline temporal-difference learning. The obtained results show that the agent is able to collect larger and faster reward by using the VDBE-Softmax exploration strategy with both Q-learning and SARSA.},
  keywords = {Boltzmann distribution,Decision making,Informatics,Markov processes,Reinforcement learning,Robots,Task analysis},
  file = {/Users/ludwig/Zotero/storage/5Q7VM9IZ/Cruz et al. - 2018 - Action Selection Methods in a Robotic Reinforcemen.pdf;/Users/ludwig/Zotero/storage/TWN8D4VJ/8625243.html}
}

@article{dabney2018DistributionalReinforcementLearning,
  title = {Distributional {{Reinforcement Learning With Quantile Regression}}},
  author = {Dabney, Will and Rowland, Mark and Bellemare, Marc and Munos, R{\'e}mi},
  year = {2018},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  issn = {2374-3468},
  doi = {10.1609/aaai.v32i1.11791},
  abstract = {In reinforcement learning (RL), an agent interacts with the environment by taking actions and observing the next state and reward. When sampled probabilistically, these state transitions, rewards, and actions can all induce randomness in the observed long-term return. Traditionally, reinforcement learning algorithms average over this randomness to estimate the value function. In this paper, we build on recent work advocating a distributional approach to reinforcement learning in which the distribution over returns is modeled explicitly instead of only estimating the mean. That is, we examine methods of learning the value distribution instead of the value function. We give results that close a number of gaps between the theoretical and algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we extend existing results to the approximate distribution setting. Second, we present a novel distributional reinforcement learning algorithm consistent with our theoretical formulation. Finally, we evaluate this new algorithm on the Atari 2600 games, observing that it significantly outperforms many of the recent improvements on DQN, including the related distributional algorithm C51.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {temporal difference learning},
  file = {/Users/ludwig/Zotero/storage/CL8VRCS7/Dabney et al. - 2018 - Distributional Reinforcement Learning With Quantil.pdf}
}

@article{deltetto2021ExploringPotentialitiesDeep,
  title = {Exploring the {{Potentialities}} of {{Deep Reinforcement Learning}} for {{Incentive-Based Demand Response}} in a {{Cluster}} of {{Small Commercial Buildings}}},
  author = {Deltetto, Davide and Coraci, Davide and Pinto, Giuseppe and Piscitelli, Marco Savino and Capozzoli, Alfonso},
  year = {2021},
  month = jan,
  journal = {Energies},
  volume = {14},
  number = {10},
  pages = {2933},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1996-1073},
  doi = {10.3390/en14102933},
  abstract = {Demand Response (DR) programs represent an effective way to optimally manage building energy demand while increasing Renewable Energy Sources (RES) integration and grid reliability, helping the decarbonization of the electricity sector. To fully exploit such opportunities, buildings are required to become sources of energy flexibility, adapting their energy demand to meet specific grid requirements. However, in most cases, the energy flexibility of a single building is typically too small to be exploited in the flexibility market, highlighting the necessity to perform analysis at a multiple-building scale. This study explores the economic benefits associated with the implementation of a Reinforcement Learning (RL) control strategy for the participation in an incentive-based demand response program of a cluster of commercial buildings. To this purpose, optimized Rule-Based Control (RBC) strategies are compared with a RL controller. Moreover, a hybrid control strategy exploiting both RBC and RL is proposed. Results show that the RL algorithm outperforms the RBC in reducing the total energy cost, but it is less effective in fulfilling DR requirements. The hybrid controller achieves a reduction in energy consumption and energy costs by respectively 7\% and 4\% compared to a manually optimized RBC, while fulfilling DR constraints during incentive-based events.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {cluster of buildings,deep reinforcement learning,demand response,energy flexibility,energy management},
  file = {/Users/ludwig/Zotero/storage/EFVRXY8A/Deltetto et al. - 2021 - Exploring the Potentialities of Deep Reinforcement.pdf}
}

@inproceedings{dhamankar2020BenchmarkingMultiAgentDeep,
  title = {Benchmarking {{Multi-Agent Deep Reinforcement Learning Algorithms}} on a {{Building Energy Demand Coordination Task}}},
  booktitle = {Proceedings of the 1st {{International Workshop}} on {{Reinforcement Learning}} for {{Energy Management}} in {{Buildings}} \& {{Cities}}},
  author = {Dhamankar, Gauraang and {Vazquez-Canteli}, Jose R. and Nagy, Zoltan},
  year = {2020},
  month = nov,
  series = {{{RLEM}}'20},
  pages = {15--19},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3427773.3427870},
  abstract = {Periods of high demand for electricity can raise electricity prices for building users. Flattening the electricity demand curve reduces can reduce costs and increase resiliency. We formulate this task as a multi-agent reinforcement learning (MA-RL) problem, to be achieved through demand response and coordination of electricity consuming agents, i.e., buildings. Bechmarks for such MA-RL problems do not exist. Here, we contribute an empirical comparison of three classes of MA-RL algorithms: independent learners, centralized critics with decentralized execution, and value factorization learners. We evaluate these algorithms on an energy coordination task in CityLearn, an Open AI Gym environment. We found independent learners with shaped rewards to be competitive with more complex algorithms. Agents with centralized critics aim to learn a rich joint critic, which may complicate the training process and cause scalability issues. Our findings indicate value factorization learners possess the coordination benefits of centralized critics and match independent learners without individualized reward shaping.},
  isbn = {978-1-4503-8193-2},
  keywords = {agent coordination,demand response,multi-agent,reinforcement learning},
  file = {/Users/ludwig/Zotero/storage/RU75TS8F/Dhamankar et al. - 2020 - Benchmarking Multi-Agent Deep Reinforcement Learni.pdf}
}

@misc{DoingGoodTogether,
  title = {Doing Good Together: How to Coordinate Effectively, and Avoid Single-Player Thinking},
  shorttitle = {Doing Good Together},
  journal = {80,000 Hours},
  abstract = {Sapiens can cooperate in flexible ways with countless numbers of strangers. That's why we rule the world, whereas ants eat our leftovers and chimps are locked up in zoos. The historian, Yuval Harari, claims in his book Sapiens that better coordination has been the key driver of human progress. He highlights innovations like language, religion, human rights, nation states and money as valuable because they improve cooperation among strangers.},
  howpublished = {https://80000hours.org/articles/coordination/},
  langid = {american},
  file = {/Users/ludwig/Zotero/storage/LEV8LUR4/coordination.html}
}

@article{fedusRevisitingFundamentalsExperience,
  title = {Revisiting {{Fundamentals}} of {{Experience Replay}}},
  author = {Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
  pages = {17},
  abstract = {Experience replay is central to off-policy algorithms in deep reinforcement learning (RL), but there remain significant gaps in our understanding. We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: the replay capacity and the ratio of learning updates to experience collected (replay ratio). Our additive and ablative studies upend conventional wisdom around experience replay - greater capacity is found to substantially increase the performance of certain algorithms, while leaving others unaffected. Counter-intuitively we show that theoretically ungrounded, uncorrected n-step returns are uniquely beneficial while other techniques confer limited benefit for sifting through larger memory. Separately, by directly controlling the replay ratio we contextualize previous observations in the literature and empirically measure its importance across three deep RL algorithms. Finally, we conclude by testing a set of hypotheses on the nature of these performance benefits.},
  langid = {english},
  file = {/Users/ludwig/Zotero/storage/P9Q8ZK3J/Fedus et al. - Revisiting Fundamentals of Experience Replay.pdf}
}

@article{ghavamzadeh2015BayesianReinforcementLearning,
  title = {Bayesian {{Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Bayesian {{Reinforcement Learning}}},
  author = {Ghavamzadeh, Mohammad and Mannor, Shie and Pineau, Joelle and Tamar, Aviv},
  year = {2015},
  month = nov,
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {8},
  number = {5-6},
  pages = {359--483},
  publisher = {{Now Publishers, Inc.}},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000049},
  abstract = {Bayesian Reinforcement Learning: A Survey},
  langid = {english},
  file = {/Users/ludwig/Zotero/storage/AXV6ZD64/ghavamzadeh2015.pdf;/Users/ludwig/Zotero/storage/YHV8C5S6/MAL-049.html}
}

@inproceedings{glatt2021CollaborativeEnergyDemand,
  title = {Collaborative Energy Demand Response with Decentralized Actor and Centralized Critic},
  booktitle = {Proceedings of the 8th {{ACM International Conference}} on {{Systems}} for {{Energy-Efficient Buildings}}, {{Cities}}, and {{Transportation}}},
  author = {Glatt, Ruben and da Silva, Felipe Leno and Soper, Braden and Dawson, William A. and Rusu, Edward and Goldhahn, Ryan A.},
  year = {2021},
  month = nov,
  pages = {333--337},
  publisher = {{ACM}},
  address = {{Coimbra Portugal}},
  doi = {10.1145/3486611.3488732},
  abstract = {The ongoing industrialization and rising technology adoption around the world are leading to ever higher energy consumption. The benefits of electrification are enormous, but the growing demand also comes with challenges with respect to associated greenhouse gas emissions. Although continuing progress in energy research has brought up new technologies in energy generation, storage, and distribution, most of those technologies focus on increasing efficiency of individual components. Work on integration and coordination abilities between individual components in micro-grids will lead to further improvements and gains in efficiency that are necessary to reduce carbon footprints and slow down climate change. To this end, the CityLearn environment provides a simulation framework that allows the control of energy components in buildings that are organized in districts. In this paper, we propose an energy management system based on the decentralized actor-critic reinforcement learning algorithm {$\mathsl{M}\mathsl{A}\mathsl{R}\mathsl{L}\mathsl{I}\mathsl{S}\mathsl{A}$} but integrate a centralized critic and call it {$\mathsl{M}\mathsl{A}\mathsl{R}\mathsl{L}\mathsl{I}\mathsl{S}\mathsl{A}\mathsl{D}\mathsl{A}\mathsl{C}\mathsl{C}$} . In this way, we are training a model to autonomously control the energy storage of individual buildings in a CityLearn district to improve demand response guided by a better informed training signal. We show performance increases over baseline control techniques for a district but also discuss the resulting action selection for individual buildings.},
  isbn = {978-1-4503-9114-6},
  langid = {english},
  file = {/Users/ludwig/Zotero/storage/VF9ALFTS/Glatt et al. - 2021 - Collaborative energy demand response with decentra.pdf}
}

@misc{haarnoja2018SoftActorCriticOffPolicy,
  title = {Soft {{Actor-Critic}}: {{Off-Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  shorttitle = {Soft {{Actor-Critic}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  year = {2018},
  month = aug,
  number = {arXiv:1801.01290},
  eprint = {1801.01290},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1801.01290},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ludwig/Zotero/storage/VRBLKMIR/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep.pdf;/Users/ludwig/Zotero/storage/LYFM6LFP/1801.html}
}

@inproceedings{huang2021MetaActorCriticFramework,
  title = {Meta {{Actor-Critic Framework}} for {{Multi-Agent Reinforcement Learning}}},
  booktitle = {2021 4th {{International Conference}} on {{Artificial Intelligence}} and {{Pattern Recognition}}},
  author = {Huang, Jiateng and Huang, Wanrong and Wu, Dan and Lan, Long},
  year = {2021},
  month = sep,
  series = {{{AIPR}} 2021},
  pages = {636--643},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3488933.3489029},
  abstract = {In recent years, multi-agent reinforcement learning has received sustained attention in the last few years. The typical Actor-Critic methods learn mappings directly from observation to action without understanding the tasks themselves. In this paper, we present a meta actor-critic framework for meta-actor critique based on observational learning processes and the additional loss of meta-learning actors to accelerate and improve multi-agent learning across agents' experiences. Within our framework, all agents are deliberately designed to share the same meta-critic loss to achieve the optimum actor learning progress. Meanwhile, by minimizing the loss of meta-actors, the meta actor learns the features of the meta-observation, leading to better actions. We implemented the MADDPG and MATD3 algorithms in our proposed framework and empirically demonstrated the superiority of our framework on two kinds of multi-agent tasks. In addition, the framework can be flexibly incorporated into various contemporary multi-agent Actor-Critic methods.},
  isbn = {978-1-4503-8408-7},
  keywords = {meta-learning,multi-agent system,Reinforcement learning},
  file = {/Users/ludwig/Zotero/storage/N4TTZQJL/Huang et al. - 2021 - Meta Actor-Critic Framework for Multi-Agent Reinfo.pdf}
}

@article{hullermeier2021AleatoricEpistemicUncertainty,
  title = {Aleatoric and Epistemic Uncertainty in Machine Learning: An Introduction to Concepts and Methods},
  shorttitle = {Aleatoric and Epistemic Uncertainty in Machine Learning},
  author = {H{\"u}llermeier, Eyke and Waegeman, Willem},
  year = {2021},
  month = mar,
  journal = {Machine Learning},
  volume = {110},
  number = {3},
  pages = {457--506},
  issn = {1573-0565},
  doi = {10.1007/s10994-021-05946-3},
  abstract = {The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.},
  langid = {english},
  file = {/Users/ludwig/Zotero/storage/XQB3DD4A/H√ºllermeier und Waegeman - 2021 - Aleatoric and epistemic uncertainty in machine lea.pdf}
}

@inproceedings{jimenez-raboso2021SinergymBuildingSimulation,
  title = {Sinergym: A Building Simulation and Control Framework for Training Reinforcement Learning Agents},
  shorttitle = {Sinergym},
  booktitle = {Proceedings of the 8th {{ACM International Conference}} on {{Systems}} for {{Energy-Efficient Buildings}}, {{Cities}}, and {{Transportation}}},
  author = {{Jim{\'e}nez-Raboso}, Javier and {Campoy-Nieves}, Alejandro and {Manjavacas-Lucas}, Antonio and {G{\'o}mez-Romero}, Juan and {Molina-Solana}, Miguel},
  year = {2021},
  month = nov,
  series = {{{BuildSys}} '21},
  pages = {319--323},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3486611.3488729},
  abstract = {We introduce Sinergym, an open-source building simulation and control framework for training reinforcement learning agents. The proposed framework is compatible with EnergyPlus models and allows to implement Python-based controllers, facilitating reproducibility of experiments and generalization to multiple scenarios. A comparison between Sinergym and other existing libraries for building control is included. We describe its design and main functionalities, such as offering a diverse set of environments with different buildings, weather types and action spaces. The provided examples show the usage of the framework for benchmarking reinforcement learning methods for building control.},
  isbn = {978-1-4503-9114-6},
  keywords = {buildings,control,reinforcement learning,simulation},
  file = {/Users/ludwig/Zotero/storage/7GUIFKEE/Jim√©nez-Raboso et al. - 2021 - Sinergym a building simulation and control framew.pdf}
}

@article{kastius2022DynamicPricingCompetition,
  title = {Dynamic Pricing under Competition Using Reinforcement Learning},
  author = {Kastius, Alexander and Schlosser, Rainer},
  year = {2022},
  month = feb,
  journal = {Journal of Revenue and Pricing Management},
  volume = {21},
  number = {1},
  pages = {50--63},
  issn = {1477-657X},
  doi = {10.1057/s41272-021-00285-3},
  abstract = {Dynamic pricing is considered a possibility to gain an advantage over competitors in modern online markets. The past advancements in Reinforcement Learning (RL) provided more capable algorithms that can be used to solve pricing problems. In this paper, we study the performance of Deep Q-Networks (DQN) and Soft Actor Critic (SAC) in different market models. We consider tractable duopoly settings, where optimal solutions derived by dynamic programming techniques can be used for verification, as well as oligopoly settings, which are usually intractable due to the curse of dimensionality. We find that both algorithms provide reasonable results, while SAC performs better than DQN. Moreover, we show that under certain conditions, RL algorithms can be forced into collusion by their competitors without direct communication.},
  langid = {english},
  keywords = {Competition,Dynamic pricing,E-commerce,Price collusion,Reinforcement learning},
  file = {/Users/ludwig/Zotero/storage/JHUZSKRE/Kastius und Schlosser - 2022 - Dynamic pricing under competition using reinforcem.pdf}
}

@article{kastius2022DynamicPricingCompetitiona,
  title = {Dynamic Pricing under Competition Using Reinforcement Learning},
  author = {Kastius, Alexander and Schlosser, Rainer},
  year = {2022},
  month = feb,
  journal = {Journal of Revenue and Pricing Management},
  volume = {21},
  number = {1},
  pages = {50--63},
  issn = {1477-657X},
  doi = {10.1057/s41272-021-00285-3},
  abstract = {Dynamic pricing is considered a possibility to gain an advantage over competitors in modern online markets. The past advancements in Reinforcement Learning (RL) provided more capable algorithms that can be used to solve pricing problems. In this paper, we study the performance of Deep Q-Networks (DQN) and Soft Actor Critic (SAC) in different market models. We consider tractable duopoly settings, where optimal solutions derived by dynamic programming techniques can be used for verification, as well as oligopoly settings, which are usually intractable due to the curse of dimensionality. We find that both algorithms provide reasonable results, while SAC performs better than DQN. Moreover, we show that under certain conditions, RL algorithms can be forced into collusion by their competitors without direct communication.},
  langid = {english},
  keywords = {Competition,Dynamic pricing,E-commerce,Price collusion,Reinforcement learning},
  file = {/Users/ludwig/Zotero/storage/AQEYMPZL/Kastius und Schlosser - 2022 - Dynamic pricing under competition using reinforcem.pdf}
}

@inproceedings{kathirgamanathan2020CentralisedSoftActor,
  title = {A {{Centralised Soft Actor Critic Deep Reinforcement Learning Approach}} to {{District Demand Side Management}} through {{CityLearn}}},
  booktitle = {Proceedings of the 1st {{International Workshop}} on {{Reinforcement Learning}} for {{Energy Management}} in {{Buildings}} \& {{Cities}}},
  author = {Kathirgamanathan, Anjukan and Twardowski, Kacper and Mangina, Eleni and Finn, Donal P.},
  year = {2020},
  month = nov,
  series = {{{RLEM}}'20},
  pages = {11--14},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3427773.3427869},
  abstract = {Reinforcement learning is a promising model-free and adaptive controller for demand side management, as part of the future smart grid, at the district level. This paper presents the results of the algorithm that was submitted for the CityLearn Challenge, which was hosted in early 2020 with the aim of designing and tuning a reinforcement learning agent to flatten and smooth the aggregated curve of electrical demand of a district of diverse buildings. The proposed solution secured second place in the challenge using a centralised 'Soft Actor Critic' deep reinforcement learning agent that was able to handle continuous action spaces. The controller was able to achieve an averaged score of 0.967 on the challenge dataset comprising of different buildings and climates. This highlights the potential application of deep reinforcement learning as a plug-and-play style controller, that is capable of handling different climates and a heterogenous building stock, for district demand side management of buildings.},
  isbn = {978-1-4503-8193-2},
  keywords = {Deep Reinforcement Learning,Demand Side Management,Smart Grid},
  file = {/Users/ludwig/Zotero/storage/HKLS5NDA/Kathirgamanathan et al. - 2020 - A Centralised Soft Actor Critic Deep Reinforcement.pdf}
}

@article{kathirgamanathan2021DatadrivenPredictiveControl,
  title = {Data-Driven Predictive Control for Unlocking Building Energy Flexibility: {{A}} Review},
  shorttitle = {Data-Driven Predictive Control for Unlocking Building Energy Flexibility},
  author = {Kathirgamanathan, Anjukan and De Rosa, Mattia and Mangina, Eleni and Finn, Donal P.},
  year = {2021},
  month = jan,
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {135},
  pages = {110120},
  issn = {1364-0321},
  doi = {10.1016/j.rser.2020.110120},
  abstract = {Managing supply and demand in the electricity grid is becoming more challenging due to the increasing penetration of variable renewable energy sources. As significant end-use consumers, and through better grid integration, buildings are expected to play an expanding role in the future smart grid. Predictive control allows buildings to better harness available energy flexibility from the building passive thermal mass. However, due to the heterogeneous nature of the building stock, developing computationally tractable control-oriented models, which adequately represent the complex and nonlinear thermal-dynamics of individual buildings, is proving to be a major hurdle. Data-driven predictive control, coupled with the ``Internet of Things'', holds the promise for a scalable and transferrable approach, with data-driven models replacing traditional physics-based models. This review examines recent work utilising data-driven predictive control for demand side management application with a special focus on the nexus of model development and control integration, which to date, previous reviews have not addressed. Further topics examined include the practical requirements for harnessing passive thermal mass and the issue of feature selection. Current research gaps are outlined and future research pathways are suggested to identify the most promising data-driven predictive control techniques for grid integration of buildings.},
  langid = {english},
  keywords = {Building energy flexibility,Data-driven,Machine learning,Model predictive control (MPC),Review,Smart grid},
  file = {/Users/ludwig/Zotero/storage/FNHPNNKP/Kathirgamanathan et al. - 2021 - Data-driven predictive control for unlocking build.pdf}
}

@article{kim2016DynamicPricingEnergy,
  title = {Dynamic {{Pricing}} and {{Energy Consumption Scheduling With Reinforcement Learning}}},
  author = {Kim, Byung-Gook and Zhang, Yu and {van der Schaar}, Mihaela and Lee, Jang-Won},
  year = {2016},
  month = sep,
  journal = {IEEE Transactions on Smart Grid},
  volume = {7},
  number = {5},
  pages = {2187--2198},
  issn = {1949-3061},
  doi = {10.1109/TSG.2015.2495145},
  abstract = {In this paper, we study a dynamic pricing and energy consumption scheduling problem in the microgrid where the service provider acts as a broker between the utility company and customers by purchasing electric energy from the utility company and selling it to the customers. For the service provider, even though dynamic pricing is an efficient tool to manage the microgrid, the implementation of dynamic pricing is highly challenging due to the lack of the customer-side information and the various types of uncertainties in the microgrid. Similarly, the customers also face challenges in scheduling their energy consumption due to the uncertainty of the retail electricity price. In order to overcome the challenges of implementing dynamic pricing and energy consumption scheduling, we develop reinforcement learning algorithms that allow each of the service provider and the customers to learn its strategy without a priori information about the microgrid. Through numerical results, we show that the proposed reinforcement learning-based dynamic pricing algorithm can effectively work without a priori information about the system dynamics and the proposed energy consumption scheduling algorithm further reduces the system cost thanks to the learning capability of each customer.},
  keywords = {Cost function,demand response,dynamic pricing,Dynamic scheduling,electricity market,Energy consumption,Heuristic algorithms,load scheduling,Markov decision process,microgrid,Microgrids,Pricing,reinforcement learning,Smart grid,Smart grids},
  file = {/Users/ludwig/Zotero/storage/PZXFYYWB/Kim et al. - 2016 - Dynamic Pricing and Energy Consumption Scheduling .pdf;/Users/ludwig/Zotero/storage/8IVTA4BN/7321806.html}
}

@inproceedings{kuss2003GaussianProcessesReinforcement,
  title = {Gaussian {{Processes}} in {{Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kuss, Malte and Rasmussen, Carl},
  year = {2003},
  volume = {16},
  publisher = {{MIT Press}},
  abstract = {We exploit some useful properties of Gaussian process (GP) regression models for reinforcement learning in continuous state spaces and dis- crete time. We demonstrate how the GP model allows evaluation of the value function in closed form. The resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space. Further, we speculate that the intrinsic ability of GP models to charac- terise distributions of functions would allow the method to capture entire distributions over future values instead of merely their expectation, which has traditionally been the focus of much of reinforcement learning.},
  file = {/Users/ludwig/Zotero/storage/5HS3YSYF/Kuss und Rasmussen - 2003 - Gaussian Processes in Reinforcement Learning.pdf}
}

@misc{LeveragePointsPlaces,
  title = {Leverage {{Points}}: {{Places}} to {{Intervene}} in a {{System}}},
  shorttitle = {Leverage {{Points}}},
  journal = {The Academy for Systems Change},
  abstract = {By Donella Meadows\textasciitilde{} Folks who do systems analysis have a great belief in ``leverage points.'' These are places within a complex system (a corporation, an economy, a living body, a city, an ecosystem) where a small shift in one thing can produce big changes in everything. This idea is not unique to systems analysis \textemdash{} [\ldots ]},
  langid = {american},
  file = {/Users/ludwig/Zotero/storage/34DW9G23/leverage-points-places-to-intervene-in-a-system.html}
}

@article{li2021EnergyFlexibilityResidential,
  title = {Energy Flexibility of Residential Buildings: {{A}} Systematic Review of Characterization and Quantification Methods and Applications},
  shorttitle = {Energy Flexibility of Residential Buildings},
  author = {Li, Han and Wang, Zhe and Hong, Tianzhen and Piette, Mary Ann},
  year = {2021},
  month = aug,
  journal = {Advances in Applied Energy},
  volume = {3},
  pages = {100054},
  issn = {2666-7924},
  doi = {10.1016/j.adapen.2021.100054},
  abstract = {With building electric demand becoming increasingly dynamic, and a growing percentage of intermittent renewable power generation from solar photovoltaics and wind turbines, the power grid is facing increasing challenge to manage the real time balance between the supply and demand. With advancements in smart sensing and metering, smart appliances, electric vehicles, and energy storage technologies, demand side management of residential buildings can help the grid to improve stability by optimizing flexible loads. This paper reviews recent studies on residential building demand side management, with a focus on characterization and quantification of energy flexibility covering various types of flexible loads, metrics, methods, and applications. The reviewed studies showed four levels of applications: building level (45\%), district or community level (29\%), system level (19\%), and building sector level (7\%). Shifting loads is the dominant flexibility type in 60\% of applications, followed by shedding (19\%), generation (16\%), and modulating (6\%). Depending on the technology and application scope, flexible operations have a wide range of performance, with peak power reductions of 1\%\textasciitilde 65\%, energy savings up to 60\%, operational cost reduction of 1\%\textasciitilde 48\%, and greenhouse gas emission reductions of up to29\%. More than half (51\%) of the studies employed control strategies to achieve flexibility; among those 72\% used optimal controls, while 28\% used rule-based controls. About 58\% of the studies used mathematical formulation to quantify energy flexibility. Most studies were based on simulation, while less than 15\% of the studies had measurements from experiments or field tests. The review reveals research opportunities to address significant gaps in the existing literature: (1) establishing a common definition and performance metrics for energy flexibility of buildings that are technology and application agnostic, (2) developing an ontology to standardize representation of flexibility resources for interoperability, (3) integrating occupant impacts into the quantification and optimization of energy flexibility, and (4) developing requirements and credits of energy flexibility in building energy codes and standards. Findings from the review can inform future research and development of energy flexible buildings which are essential to a reliable and resilient power grid.},
  langid = {english},
  keywords = {Demand-side management,Energy flexibility,Flexibility characteristics,Metrics,Residential buildings},
  file = {/Users/ludwig/Zotero/storage/V4F7BVE8/Li et al. - 2021 - Energy flexibility of residential buildings A sys.pdf;/Users/ludwig/Zotero/storage/56TNQQ9I/S2666792421000469.html}
}

@article{ludwigDataDrivenMethodsDemandSide,
  title = {Data-{{Driven Methods}} for {{Demand-Side Flexibility}} in {{Energy Systems}}},
  author = {Ludwig, Nicole Nadine},
  pages = {176},
  langid = {english},
  file = {/Users/ludwig/Zotero/storage/XGGE94U2/Ludwig - Data-Driven Methods for Demand-Side Flexibility in.pdf}
}

@inproceedings{lutjens2019SafeReinforcementLearning,
  title = {Safe {{Reinforcement Learning With Model Uncertainty Estimates}}},
  booktitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {L{\"u}tjens, Bj{\"o}rn and Everett, Michael and How, Jonathan P.},
  year = {2019},
  month = may,
  pages = {8662--8668},
  issn = {2577-087X},
  doi = {10.1109/ICRA.2019.8793611},
  abstract = {Many current autonomous systems are being designed with a strong reliance on black box predictions from deep neural networks (DNNs). However, DNNs tend to be overconfident in predictions on unseen data and can give unpredictable results for far-from-distribution test data. The importance of predictions that are robust to this distributional shift is evident for safety-critical applications, such as collision avoidance around pedestrians. Measures of model uncertainty can be used to identify unseen data, but the state-of-the-art extraction methods such as Bayesian neural networks are mostly intractable to compute. This paper uses MC-Dropout and Bootstrapping to give computationally tractable and parallelizable uncertainty estimates. The methods are embedded in a Safe Reinforcement Learning framework to form uncertainty-aware navigation around pedestrians. The result is a collision avoidance policy that knows what it does not know and cautiously avoids pedestrians that exhibit unseen behavior. The policy is demonstrated in simulation to be more robust to novel observations and take safer actions than an uncertainty-unaware baseline.},
  keywords = {Collision avoidance,Computational modeling,Data models,Neural networks,Reinforcement learning,Training,Uncertainty},
  file = {/Users/ludwig/Zotero/storage/SD9FDLEE/L√ºtjens et al. - 2019 - Safe Reinforcement Learning With Model Uncertainty.pdf;/Users/ludwig/Zotero/storage/9H6B6596/8793611.html}
}

@article{meinshausenQuantileRegressionForests,
  title = {Quantile {{Regression Forests}}},
  author = {Meinshausen, Nicolai},
  pages = {17},
  abstract = {Random forests were introduced as a machine learning tool in Breiman (2001) and have since proven to be very popular and powerful for high-dimensional regression and classification. For regression, random forests give an accurate approximation of the conditional mean of a response variable. It is shown here that random forests provide information about the full conditional distribution of the response variable, not only about the conditional mean. Conditional quantiles can be inferred with quantile regression forests, a generalisation of random forests. Quantile regression forests give a non-parametric and accurate way of estimating conditional quantiles for high-dimensional predictor variables. The algorithm is shown to be consistent. Numerical examples suggest that the algorithm is competitive in terms of predictive power.},
  langid = {english},
  file = {/Users/ludwig/Zotero/storage/6P3CE7YN/Meinshausen - Quantile Regression Forests.pdf}
}

@article{mengelkamp2018DesigningMicrogridEnergy,
  title = {Designing Microgrid Energy Markets: {{A}} Case Study: {{The Brooklyn Microgrid}}},
  shorttitle = {Designing Microgrid Energy Markets},
  author = {Mengelkamp, Esther and G{\"a}rttner, Johannes and Rock, Kerstin and Kessler, Scott and Orsini, Lawrence and Weinhardt, Christof},
  year = {2018},
  month = jan,
  journal = {Applied Energy},
  volume = {210},
  pages = {870--880},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2017.06.054},
  abstract = {Generation from distributed renewable energy sources is constantly increasing. Due to its volatility, the integration of this non-controllable generation poses severe challenges to the current energy system. Thus, ensuring a reliable balance of energy generation and consumption becomes increasingly demanding. In our approach to tackle these challenges, we suggest that consumers and prosumers can trade self-produced energy in a peer-to-peer fashion on microgrid energy markets. Thus, consumers and prosumers can keep profits from energy trading within their community. This provides incentives for investments in renewable generation plants and for locally balancing supply and demand. Hence, both financial as well as socio-economic incentives for the integration and expansion of locally produced renewable energy are provided. The efficient operation of these microgrid energy markets requires innovative information systems for integrating the market participants in a user-friendly and comprehensive way. To this end, we present the concept of a blockchain-based microgrid energy market without the need for central intermediaries. We derive seven market components as a framework for building efficient microgrid energy markets. Then, we evaluate the Brooklyn Microgrid project as a case study of such a market according to the required components. We show that the Brooklyn Microgrid fully satisfies three and partially fulfills an additional three of the seven components. Furthermore, the case study demonstrates that blockchains are an eligible technology to operate decentralized microgrid energy markets. However, current regulation does not allow to run local peer-to-peer energy markets in most countries and, hence, the seventh component cannot be satisfied yet.},
  langid = {english},
  keywords = {Blockchain,Case study,Market design,Microgrid energy market,Peer-to-peer trading,Renewable energy},
  file = {/Users/ludwig/Zotero/storage/YVX9HNW8/Mengelkamp et al. - 2018 - Designing microgrid energy markets A case study .pdf;/Users/ludwig/Zotero/storage/D3D7HH5F/S030626191730805X.html}
}

@inproceedings{metelli2019PropagatingUncertaintyReinforcement,
  title = {Propagating {{Uncertainty}} in {{Reinforcement Learning}} via {{Wasserstein Barycenters}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Metelli, Alberto Maria and Likmeta, Amarildo and Restelli, Marcello},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  abstract = {How does the uncertainty of the value function propagate when performing temporal difference learning? In this paper, we address this question by proposing a Bayesian framework in which we employ approximate posterior distributions to model the uncertainty of the value function and Wasserstein barycenters to propagate it across state-action pairs. Leveraging on these tools, we present an algorithm, Wasserstein Q-Learning (WQL), starting in the tabular case and then, we show how it can be extended to deal with continuous domains. Furthermore, we prove that, under mild assumptions, a slight variation of WQL enjoys desirable theoretical properties in the tabular setting. Finally, we present an experimental campaign to show the effectiveness of WQL on finite problems, compared to several RL algorithms, some of which are specifically designed for exploration, along with some preliminary results on Atari games.},
  file = {/Users/ludwig/Zotero/storage/FGRWX3WQ/Metelli et al. - 2019 - Propagating Uncertainty in Reinforcement Learning .pdf}
}

@article{mnih2015HumanlevelControlDeep,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  abstract = {An artificial agent is developed that learns to play~a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a~performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Computer science},
  file = {/Users/ludwig/Zotero/storage/D4RIXRY9/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf}
}

@inproceedings{nagy2021CitylearnChallenge2021,
  title = {The Citylearn Challenge 2021},
  booktitle = {Proceedings of the 8th {{ACM International Conference}} on {{Systems}} for {{Energy-Efficient Buildings}}, {{Cities}}, and {{Transportation}}},
  author = {Nagy, Zoltan and {V{\'a}zquez-Canteli}, Jos{\'e} R. and Dey, Sourav and Henze, Gregor},
  year = {2021},
  month = nov,
  series = {{{BuildSys}} '21},
  pages = {218--219},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3486611.3492226},
  abstract = {We present the results of The CityLearn Challenge 2021. Five teams competed over six months to design the best multi-agent reinforcement learning agent for the energy management of a microgrid of nine buildings.},
  isbn = {978-1-4503-9114-6},
  file = {/Users/ludwig/Zotero/storage/NS8ZAZ7D/Nagy et al. - 2021 - The citylearn challenge 2021.pdf}
}

@article{narayanamurthy2016GridIntegrationZero,
  title = {Grid {{Integration}} of {{Zero Net Energy Communities}}},
  author = {Narayanamurthy, Ram and Handa, Rachna and Tumilowicz, Nick and Herro, C R and Shah, Sunil},
  year = {2016},
  journal = {ACEEE Summer Study Energy Effic. Build},
  abstract = {Zero Net Energy (ZNE) homes will be the standard for residential construction in California by 2020. The Electric Power Research Institute (EPRI) worked with builder Meritage Homes, the California Public Utilities Commission (CPUC), Southern California Edison (SCE), and BIRAenergy, to design, construct, sell, and monitor a neighborhood of ZNE homes in Fontana, CA. These homes saturated two distribution transformers, to simulate the expected scenario of 2020 when every home would be ZNE. These homes were built to the California code specification of zero TDV (Time Dependent Value) Energy. They utilized electric heating and water heating, with an eye to meeting long term GHG goals through electrification. ZNE homes do not translate to zero energy use on a monthly, daily or hourly basis, and could actually increase the investments required in the transmission and distribution system. Their load profiles mimic the ``duck curve'', and at volume upset grid balancing with excess generation during the mid-morning hours and steep evening ramps. The research conducted evaluates potential of controllable loads and energy storage to balance the distribution grid at the transformer, feeder and substation levels. The initial results indicate that grid planning, traditionally based on cooling would need to increase wiring and transformers to accommodate ZNE with newer loads like heat pumps and electric vehicles. The analysis indicated more energy efficiency within ZNE assists both the grid and the builder, while PV is non-coincident. A load aggregation platform that connects and controls loads, battery storage, and PV generation can potentially reduce electrical infrastructure.},
  langid = {english},
  file = {/Users/ludwig/Zotero/storage/A9RGRQI2/Narayanamurthy et al. - Grid Integration of Zero Net Energy Communities.pdf}
}

@inproceedings{nikolov2022InformationDirectedExplorationDeep,
  title = {Information-{{Directed Exploration}} for {{Deep Reinforcement Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Nikolov, Nikolay and Kirschner, Johannes and Berkenkamp, Felix and Krause, Andreas},
  year = {2022},
  month = feb,
  abstract = {Efficient exploration remains a major challenge for reinforcement learning. One reason is that the variability of the returns often depends on the current state and action, and is therefore heteroscedastic. Classical exploration strategies such as upper confidence bound algorithms and Thompson sampling fail to appropriately account for heteroscedasticity, even in the bandit setting. Motivated by recent findings that address this issue in bandits, we propose to use Information-Directed Sampling (IDS) for exploration in reinforcement learning. As our main contribution, we build on recent advances in distributional reinforcement learning and propose a novel, tractable approximation of IDS for deep Q-learning. The resulting exploration strategy explicitly accounts for both parametric uncertainty and heteroscedastic observation noise. We evaluate our method on Atari games and demonstrate a significant improvement over alternative approaches.},
  langid = {english},
  file = {/Users/ludwig/Zotero/storage/9HFQ5ERU/Nikolov et al. - 2022 - Information-Directed Exploration for Deep Reinforc.pdf}
}

@article{norouzi2022ReviewSociotechnicalBarriers,
  title = {A Review of Socio-Technical Barriers to {{Smart Microgrid}} Development},
  author = {Norouzi, Farshid and Hoppe, Thomas and Elizondo, Laura Ramirez and Bauer, Pavol},
  year = {2022},
  month = oct,
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {167},
  pages = {112674},
  issn = {1364-0321},
  doi = {10.1016/j.rser.2022.112674},
  abstract = {Smart MicroGrids (SMGs) can be seen as a promising option when it comes to addressing the urgent need for sustainable transition in electric systems from the current fossil fuel-based centralised system to a low-carbon, renewable-based decentralised system. Unlike previous studies that were restricted to a limited number of actors and only took a mono-disciplinary research approach, this current review adopts a multidisciplinary, socio-technical approach and addresses the factors that have been hindering the development of SMGs and considers how these barriers interact. This study contributes to the body of literature on the development of SMGs by mapping and discerning technical, regulatory, market, social and institutional barriers for different types of actors, including technology providers, consumers, Distributed Generation (DG) providers and system operators, based on information derived from laboratory reports, demonstration pilots, and academic journals. In addition, attention is paid to how these barriers interact based on real-life experimentation. A holistic picture of barriers and their interaction is presented as well as recommendations for future research.},
  langid = {english},
  keywords = {Barriers,Multidisciplinary approach,Smart MicroGrids,Socio-technical,Sustainable transition},
  file = {/Users/ludwig/Zotero/storage/YRUSV6ME/Norouzi et al. - 2022 - A review of socio-technical barriers to Smart Micr.pdf;/Users/ludwig/Zotero/storage/V9RKZMDV/S1364032122005640.html}
}

@article{nweye2022RealworldChallengesMultiagent,
  title = {Real-World Challenges for Multi-Agent Reinforcement Learning in Grid-Interactive Buildings},
  author = {Nweye, Kingsley and Liu, Bo and Stone, Peter and Nagy, Zoltan},
  year = {2022},
  month = nov,
  journal = {Energy and AI},
  volume = {10},
  pages = {100202},
  issn = {2666-5468},
  doi = {10.1016/j.egyai.2022.100202},
  abstract = {Building upon prior research that highlighted the need for standardizing environments for building control research, and inspired by recently introduced challenges for real life reinforcement learning (RL) control, here we propose a non-exhaustive set of nine real world challenges for RL control in grid-interactive buildings (GIBs). We argue that research in this area should be expressed in this framework in addition to providing a standardized environment for repeatability. Advanced controllers such as model predictive control (MPC) and RL control have both advantages and disadvantages that prevent them from being implemented in real world problems. Comparisons between the two are rare, and often biased. By focusing on the challenges, we can investigate the performance of the controllers under a variety of situations and generate a fair comparison. As a demonstration, we implement the offline learning challenge in CityLearn, an OpenAI Gym environment for the easy implementation of RL agents in a demand response setting to reshape the aggregated curve of electricity demand by controlling the energy storage of a diverse set of buildings in a district. We use CityLearn to study the impact of different levels of domain knowledge and complexity of RL algorithms and show that the sequence of operations (SOOs) utilized in a rule based controller (RBC) that provides fixed logs to RL agents during offline training affect the performance of the agents when evaluated on a set of four energy flexibility metrics. Longer offline training from an optimized RBC leads to improved performance in the long run. RL agents that train on the logs from a simplified RBC risk poorer performance as the offline training period increases. We also observe no impact on performance from information sharing amongst agents. We call for a more interdisciplinary effort of the research community to address the real world challenges, and unlock the potential of GIB controllers.},
  langid = {english},
  keywords = {Benchmarking,Grid-interactive buildings,Reinforcement learning},
  file = {/Users/ludwig/Zotero/storage/8YFTLDP2/Nweye et al. - 2022 - Real-world challenges for multi-agent reinforcemen.pdf;/Users/ludwig/Zotero/storage/VRRTBCXR/S2666546822000489.html}
}

@inproceedings{osband2013MoreEfficientReinforcement,
  title = {({{More}}) {{Efficient Reinforcement Learning}} via {{Posterior Sampling}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Osband, Ian and Russo, Daniel and Van Roy, Benjamin},
  year = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/ludwig/Zotero/storage/TH72TPE4/Osband et al. - 2013 - (More) Efficient Reinforcement Learning via Poster.pdf}
}

@article{perera2020QuantifyingImpactsClimate,
  title = {Quantifying the Impacts of Climate Change and Extreme Climate Events on Energy Systems},
  author = {Perera, A. T. D. and Nik, Vahid M. and Chen, Deliang and Scartezzini, Jean-Louis and Hong, Tianzhen},
  year = {2020},
  month = feb,
  journal = {Nature Energy},
  volume = {5},
  number = {2},
  pages = {150--159},
  publisher = {{Nature Publishing Group}},
  issn = {2058-7546},
  doi = {10.1038/s41560-020-0558-0},
  abstract = {Climate induced extreme weather events and weather variations will affect both the demand of energy and the resilience of energy supply systems. The specific potential impact of extreme events on energy systems has been difficult to quantify due to the unpredictability of future weather events. Here we develop a stochastic-robust optimization method to consider both low impact variations and extreme events. Applications of the method to 30 cities in Sweden, by considering 13 climate change scenarios, reveal that uncertainties in renewable energy potential and demand can lead to a significant performance gap (up to 34\% for grid integration) brought by future climate variations and a drop in power supply reliability (up to 16\%) due to extreme weather events. Appropriate quantification of the climate change impacts will ensure robust operation of the energy systems and enable renewable energy penetration above 30\% for a majority of the cities.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Power stations,Solar energy,Wind energy},
  file = {/Users/ludwig/Zotero/storage/KCJ2QCX9/Perera et al. - 2020 - Quantifying the impacts of climate change and extr.pdf;/Users/ludwig/Zotero/storage/FDLRJYMT/s41560-020-0558-0.html}
}

@article{philipsen2015EffectsGroupPsychotherapy,
  title = {Effects of {{Group Psychotherapy}}, {{Individual Counseling}}, {{Methylphenidate}}, and {{Placebo}} in the {{Treatment}} of {{Adult Attention-Deficit}}/{{Hyperactivity Disorder}}: {{A Randomized Clinical Trial}}},
  shorttitle = {Effects of {{Group Psychotherapy}}, {{Individual Counseling}}, {{Methylphenidate}}, and {{Placebo}} in the {{Treatment}} of {{Adult Attention-Deficit}}/{{Hyperactivity Disorder}}},
  author = {Philipsen, Alexandra and Jans, Thomas and Graf, Erika and Matthies, Swantje and Borel, Patricia and Colla, Michael and Gentschow, Laura and Langner, Daina and Jacob, Christian and {Gro{\ss}-Lesch}, Silke and Sobanski, Esther and Alm, Barbara and {Schumacher-Stien}, Martina and Roesler, Michael and Retz, Wolfgang and {Retz-Junginger}, Petra and Kis, Bernhard and {Abdel-Hamid}, Mona and Heinrich, Viola and Huss, Michael and Kornmann, Catherine and B{\"u}rger, Arne and Perlov, Evgeniy and Ihorst, Gabriele and Schlander, Michael and Berger, Mathias and {Tebartz van Elst}, Ludger and {Comparison of Methylphenidate and Psychotherapy in Adult ADHD Study (COMPAS) Consortium}},
  year = {2015},
  month = dec,
  journal = {JAMA psychiatry},
  volume = {72},
  number = {12},
  pages = {1199--1210},
  issn = {2168-6238},
  doi = {10.1001/jamapsychiatry.2015.2146},
  abstract = {IMPORTANCE: Attention-deficit/hyperactivity disorder (ADHD) is a neurodevelopmental disorder with high prevalence in adulthood. There is a recognized need to assess the efficacy of psychotherapy in adult ADHD. OBJECTIVE: To evaluate the efficacy of cognitive behavioral group psychotherapy (GPT) compared with individual clinical management (CM) and that of methylphenidate hydrochloride compared with placebo. DESIGN, SETTING, AND PARTICIPANTS: Prospective, multicenter, randomized clinical trial of 18- to 58-year-old outpatients with ADHD from 7 German study centers. Patients were recruited between January 2007 and August 2010, treatment was finalized in August 2011, and final follow-up assessments occurred in March 2013. INTERVENTIONS: Sessions of GPT and CM were held weekly for the first 12 weeks and monthly thereafter (9 months). Patients received either methylphenidate or placebo for 1 year. MAIN OUTCOMES AND MEASURES: The primary outcome was the change in the ADHD Index of the Conners Adult ADHD Rating Scale from baseline to the end of the 3-month intensive treatment (blinded observer ratings). Secondary outcomes included ADHD ratings after 1 year, blinded observer ratings using the Clinical Global Impression Scale, and self-ratings of depression. RESULTS: Among 1480 prescreened patients, 518 were assessed for eligibility, 433 were centrally randomized, and 419 were analyzed as randomized. After 3 months, the ADHD Index all-group baseline mean of 20.6 improved to adjusted means of 17.6 for GPT and 16.5 for CM, with no significant difference between groups. Methylphenidate (adjusted mean, 16.2) was superior to placebo (adjusted mean, 17.9) (difference, -1.7; 97.5\% CI, -3.0 to -0.4; P\,=\,.003). After 1 year, treatment effects remained essentially stable. Descriptive analyses showed that methylphenidate was superior to placebo in patients assigned to GPT (difference, -1.7; 95\% CI, -3.2 to -0.1; P\,=\,.04) or CM (difference, -1.7; 95\% CI, -3.3 to -0.2; P\,=\,.03). Regarding depression, no significant differences were found. In contrast, GPT was superior to CM for all visits in the Clinical Global Impression global assessment of effectiveness. CONCLUSION AND RELEVANCE: Highly structured group intervention did not outperform individual CM with regard to the primary outcome. Psychological interventions resulted in better outcomes during a 1-year period when combined with methylphenidate as compared with placebo. TRIAL REGISTRATION: isrctn.org Identifier: ISRCTN54096201.},
  langid = {english},
  pmid = {26536057},
  keywords = {Adolescent,Adult,Attention Deficit Disorder with Hyperactivity,Central Nervous System Stimulants,Cognitive Behavioral Therapy,Combined Modality Therapy,Counseling,Double-Blind Method,Female,Humans,Male,Methylphenidate,Middle Aged,Psychotherapy; Group,Young Adult},
  file = {/Users/ludwig/Zotero/storage/SWHQ6N8A/Philipsen et al. - 2015 - Effects of Group Psychotherapy, Individual Counsel.pdf}
}

@misc{pigott2021GridLearnMultiagentReinforcement,
  title = {{{GridLearn}}: {{Multiagent Reinforcement Learning}} for {{Grid-Aware Building Energy Management}}},
  shorttitle = {{{GridLearn}}},
  author = {Pigott, Aisling and Crozier, Constance and Baker, Kyri and Nagy, Zoltan},
  year = {2021},
  month = oct,
  number = {arXiv:2110.06396},
  eprint = {2110.06396},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2110.06396},
  abstract = {Increasing amounts of distributed generation in distribution networks can provide both challenges and opportunities for voltage regulation across the network. Intelligent control of smart inverters and other smart building energy management systems can be leveraged to alleviate these issues. GridLearn is a multiagent reinforcement learning platform that incorporates both building energy models and power flow models to achieve grid level goals, by controlling behind-the-meter resources. This study demonstrates how multi-agent reinforcement learning can preserve building owner privacy and comfort while pursuing grid-level objectives. Building upon the CityLearn framework which considers RL for building-level goals, this work expands the framework to a network setting where grid-level goals are additionally considered. As a case study, we consider voltage regulation on the IEEE-33 bus network using controllable building loads, energy storage, and smart inverters. The results show that the RL agents nominally reduce instances of undervoltages and reduce instances of overvoltages by 34\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems},
  file = {/Users/ludwig/Zotero/storage/4SSJIIIT/Pigott et al. - 2021 - GridLearn Multiagent Reinforcement Learning for G.pdf;/Users/ludwig/Zotero/storage/HXZLGN65/2110.html}
}

@article{pinto2021CoordinatedEnergyManagement,
  title = {Coordinated Energy Management for a Cluster of Buildings through Deep Reinforcement Learning},
  author = {Pinto, Giuseppe and Piscitelli, Marco Savino and {V{\'a}zquez-Canteli}, Jos{\'e} Ram{\'o}n and Nagy, Zolt{\'a}n and Capozzoli, Alfonso},
  year = {2021},
  month = aug,
  journal = {Energy},
  volume = {229},
  pages = {120725},
  issn = {0360-5442},
  doi = {10.1016/j.energy.2021.120725},
  abstract = {Advanced control strategies can enable energy flexibility in buildings by enhancing on-site renewable energy exploitation and storage operation, significantly reducing both energy costs and emissions. However, when the energy management is faced shifting from a single building to a cluster of buildings, uncoordinated strategies may have negative effects on the grid reliability, causing undesirable new peaks. To overcome these limitations, the paper explores the opportunity to enhance energy flexibility of a cluster of buildings, taking advantage from the mutual collaboration between single buildings by pursuing a coordinated approach in energy management. This is achieved using Deep Reinforcement Learning (DRL), an adaptive model-free control algorithm, employed to manage the thermal storages of a cluster of four buildings equipped with different energy systems. The controller was designed to flatten the cluster load profile while optimizing energy consumption of each building. The coordinated energy management controller is tested and compared against a manually optimised rule-based one. Results shows a reduction of operational costs of about 4\%, together with a decrease of peak demand up to 12\%. Furthermore, the control strategy allows to reduce the average daily peak and average peak-to-average ratio by 10 and 6\% respectively, highlighting the benefits of a coordinated approach.},
  langid = {english},
  keywords = {Building energy flexibility,Coordinated energy management,Deep reinforcement learning,Grid interaction,Peak demand reduction},
  file = {/Users/ludwig/Zotero/storage/7QTS6ULQ/Pinto et al. - 2021 - Coordinated energy management for a cluster of bui.pdf}
}

@article{pinto2022EnhancingEnergyManagement,
  title = {Enhancing Energy Management in Grid-Interactive Buildings: {{A}} Comparison among Cooperative and Coordinated Architectures},
  shorttitle = {Enhancing Energy Management in Grid-Interactive Buildings},
  author = {Pinto, Giuseppe and Kathirgamanathan, Anjukan and Mangina, Eleni and Finn, Donal P. and Capozzoli, Alfonso},
  year = {2022},
  month = mar,
  journal = {Applied Energy},
  volume = {310},
  pages = {118497},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2021.118497},
  abstract = {The increasing penetration of renewable energy sources has the potential to contribute towards the decarbonisation of the building energy sector. However, this transition brings its own challenges including that of energy integration and potential grid instability issues arising due the stochastic nature of variable renewable energy sources. One potential approach to address these issues is demand side management, which is increasingly seen as a promising solution to improve grid stability. This is achieved by exploiting demand flexibility and shifting peak demand towards periods of peak renewable energy generation. However, the energy flexibility of a single building needs to be coordinated with other buildings to be used in a flexibility market. In this context, multi-agent systems represent a promising tool for improving the energy management of buildings at the district and grid scale. The present research formulates the energy management of four buildings equipped with thermal energy storage and PV systems as a multi-agent problem. Two multi-agent reinforcement learning methods are explored: a centralised (coordinated) controller and a decentralised (cooperative) controller, which are benchmarked against a rule-based controller. The two controllers were tested for three different climates, outperforming the rule-based controller by 3\% and 7\% respectively for cost, and 10\% and 14\% respectively for peak demand. The study shows that the multi-agent cooperative approach may be more suitable for districts with heterogeneous objectives within the individual buildings.},
  langid = {english},
  keywords = {Building energy flexibility,Deep Reinforcement Learning (DRL),Grid-interactive buildings,Multi Agent Reinforcement Learning (MARL),Soft Actor Critic (SAC)},
  file = {/Users/ludwig/Zotero/storage/UA9ICCCP/S0306261921017128.html}
}

@article{pinto2022TransferLearningSmart,
  title = {Transfer Learning for Smart Buildings: {{A}} Critical Review of Algorithms, Applications, and Future Perspectives},
  shorttitle = {Transfer Learning for Smart Buildings},
  author = {Pinto, Giuseppe and Wang, Zhe and Roy, Abhishek and Hong, Tianzhen and Capozzoli, Alfonso},
  year = {2022},
  month = feb,
  journal = {Advances in Applied Energy},
  volume = {5},
  pages = {100084},
  issn = {2666-7924},
  doi = {10.1016/j.adapen.2022.100084},
  abstract = {Smart buildings play a crucial role toward decarbonizing society, as globally buildings emit about one-third of greenhouse gases. In the last few years, machine learning has achieved a notable momentum that, if properly harnessed, may unleash its potential for advanced analytics and control of smart buildings, enabling the technique to scale up for supporting the decarbonization of the building sector. In this perspective, transfer learning aims to improve the performance of a target learner exploiting knowledge in related environments. The present work provides a comprehensive overview of transfer learning applications in smart buildings, classifying and analyzing 77 papers according to their applications, algorithms, and adopted metrics. The study identified four main application areas of transfer learning: (1) building load prediction, (2) occupancy detection and activity recognition, (3) building dynamics modeling, and (4) energy systems control. Furthermore, the review highlighted the role of deep learning in transfer learning applications that has been used in more than half of the analyzed studies. The paper also discusses how to integrate transfer learning in a smart building's ecosystem, identifying, for each application area, the research gaps and guidelines for future research directions.},
  langid = {english},
  keywords = {Building energy management,Deep learning,Machine learning,Smart buildings,Transfer learning},
  file = {/Users/ludwig/Zotero/storage/G8LHW93C/Pinto et al. - 2022 - Transfer learning for smart buildings A critical .pdf;/Users/ludwig/Zotero/storage/FFUV2TUK/S2666792422000026.html}
}

@misc{RecipeTrainingNeural,
  title = {A {{Recipe}} for {{Training Neural Networks}}},
  howpublished = {https://karpathy.github.io/2019/04/25/recipe/},
  file = {/Users/ludwig/Zotero/storage/E48K6QVG/recipe.html}
}

@article{ritchie2022Energy,
  title = {Energy},
  author = {Ritchie, Hannah and Roser, Max and Rosado, Pablo},
  year = {2022},
  month = oct,
  journal = {Our World in Data},
  abstract = {Germany: Many of us want an overview of how much energy our country consumes, where it comes from, and if we're making progress on decarbonizing our energy mix. This page provides the data for your chosen country across all of the key metrics on this topic.},
  file = {/Users/ludwig/Zotero/storage/SJTD3I7S/germany.html}
}

@misc{saxe2014ExactSolutionsNonlinear,
  title = {Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks},
  author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
  year = {2014},
  month = feb,
  number = {arXiv:1312.6120},
  eprint = {1312.6120},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, q-bio, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1312.6120},
  abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/ludwig/Zotero/storage/Q49T6DJF/Saxe et al. - 2014 - Exact solutions to the nonlinear dynamics of learn.pdf;/Users/ludwig/Zotero/storage/5FHCSCZG/1312.html}
}

@article{saxe2014ExactSolutionsNonlineara,
  title = {Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks},
  author = {Saxe, A. and McClelland, J. and Ganguli, S.},
  year = {2014},
  journal = {Proceedings of the International Conference on Learning Represenatations 2014},
  publisher = {{International Conference on Learning Represenatations 2014}},
  abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
  langid = {english},
  file = {/Users/ludwig/Zotero/storage/IJQ9NS96/Saxe et al. - 2014 - Exact solutions to the nonlinear dynamics of learn.pdf}
}

@article{schreiber2020ApplicationTwoPromising,
  title = {Application of Two Promising {{Reinforcement Learning}} Algorithms for Load Shifting in a Cooling Supply System},
  author = {Schreiber, Thomas and Eschweiler, S{\"o}ren and Baranski, Marc and M{\"u}ller, Dirk},
  year = {2020},
  month = dec,
  journal = {Energy and Buildings},
  volume = {229},
  pages = {110490},
  issn = {0378-7788},
  doi = {10.1016/j.enbuild.2020.110490},
  abstract = {With the increasing use of volatile renewable energies, the requirements for building automation and control systems (BACS) are increasing. Load shifting within local energy systems stabilizes fluctuations in the grid and can be triggered by price signals. The energy purchase can thus be considered and solved as an optimal control problem. Classical approaches, often based on the optimization of mathematical models, are uneconomical in many cases, due to the high effort involved in the model creation. Algorithms from the field of Reinforcement Learning (RL), on the other hand, have a high potential for the automation of energy system optimization, due to their model-free and data-driven characteristics. However, there is still a lack of studies that examine algorithms for BACS-related applications in a structured way. Therefore, we present a study, investigating the potential of two different RL algorithms for load shifting in a cooling supply system. We combine the benefits of Modelica, a powerful modeling language, with RL algorithms and demonstrate how generalized relationships and control decisions can be learned. The case study is modeled according to a cooling supply system in Berlin, Germany. The two different algorithms (DQN and DDPG) are used to control the operation parameters of a central compression chiller, with respect to a price signal. While real monitoring data are used as exogenous influences, the thermal dynamics of the cooling network are simulated. With the learned policies, flexibility in the network is used which leads on average to weekly cost savings of 14 \%, compared to direct load coverage. Our results suggest that, under certain conditions, RL is a suitable alternative to established methods. However, we also acknowledge that there are still research questions to address before RL can be applied in real BACS.},
  langid = {english},
  keywords = {Building automation and control,Load shifting,Optimal control,Reinforcement Learning,Simulation,Thermal systems},
  file = {/Users/ludwig/Zotero/storage/NDNXN84E/Schreiber et al. - 2020 - Application of two promising Reinforcement Learnin.pdf;/Users/ludwig/Zotero/storage/26XXTF5N/S0378778820320922.html}
}

@misc{shankar2022OperationalizingMachineLearning,
  title = {Operationalizing {{Machine Learning}}: {{An Interview Study}}},
  shorttitle = {Operationalizing {{Machine Learning}}},
  author = {Shankar, Shreya and Garcia, Rolando and Hellerstein, Joseph M. and Parameswaran, Aditya G.},
  year = {2022},
  month = sep,
  number = {arXiv:2209.09125},
  eprint = {2209.09125},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.09125},
  abstract = {Organizations rely on machine learning engineers (MLEs) to operationalize ML, i.e., deploy and maintain ML pipelines in production. The process of operationalizing ML, or MLOps, consists of a continual loop of (i) data collection and labeling, (ii) experimentation to improve ML performance, (iii) evaluation throughout a multi-staged deployment process, and (iv) monitoring of performance drops in production. When considered together, these responsibilities seem staggering -- how does anyone do MLOps, what are the unaddressed challenges, and what are the implications for tool builders? We conducted semi-structured ethnographic interviews with 18 MLEs working across many applications, including chatbots, autonomous vehicles, and finance. Our interviews expose three variables that govern success for a production ML deployment: Velocity, Validation, and Versioning. We summarize common practices for successful ML experimentation, deployment, and sustaining production performance. Finally, we discuss interviewees' pain points and anti-patterns, with implications for tool design.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/Users/ludwig/Zotero/storage/QKMHSIBY/Shankar et al. - 2022 - Operationalizing Machine Learning An Interview St.pdf;/Users/ludwig/Zotero/storage/LDM4AZF6/2209.html}
}

@book{sutton2018ReinforcementLearningIntroduction,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  langid = {english},
  lccn = {Q325.6 .R45 2018},
  keywords = {Reinforcement learning},
  file = {/Users/ludwig/Zotero/storage/XBGU2L6H/Sutton und Barto - 2018 - Reinforcement learning an introduction.pdf}
}

@article{talha2022OptimalManagementCommunity,
  title = {Optimal {{Management}} of Community {{Demand Response}}},
  author = {Talha, Ahmed Abdelrahim Mohamed},
  year = {2022},
  month = jan,
  abstract = {More than one-third of the electricity produced globally is consumed by the residential sectors [1], with nearly 17\% of CO2 emissions, are coming from residential buildings according to reports from 2018 [2] [3]. In order to cope with increase in electricity demand and consumption, while considering the environmental impacts, electricity providers are seeking to implement solutions to help them balance the supply with the electricity demand while mitigating emissions. Thus, increasing the number of conventional generation units and using unreliable renewable source of energy is not a viable investment. That's why, in recent years research attention has shifted to demand side solutions [4]. This research investigates the optimal management for an urban residential community, that can help in reducing energy consumption and peak and CO2 emissions. This will help to put an agreement with the grid operator for an agreed load shape, for efficient demand response (DR) program implementation. This work uses a framework known as CityLearn [2]. It is based on a Machine Learning branch known as Reinforcement Learning (RL), and it is used to test a variety of intelligent agents for optimizing building load consumption and load shape. The RL agent is used for controlling hot water and chilled water storages, as well as the battery system. When compared to the regular building usage, the results demonstrate that utilizing an RL agent for storage system control can be helpful, as the electricity consumption is greatly reduced when it's compared to the normal building consumption.},
  copyright = {openAccess},
  langid = {english},
  annotation = {Accepted: 2022-03-30T11:12:28Z},
  file = {/Users/ludwig/Zotero/storage/GC7LFKEW/Talha - 2022 - Optimal Management of community Demand Response.pdf;/Users/ludwig/Zotero/storage/HYSRZS52/6882.html}
}

@article{thompson1933LikelihoodThatOne,
  title = {On the {{Likelihood}} That {{One Unknown Probability Exceeds Another}} in {{View}} of the {{Evidence}} of {{Two Samples}}},
  author = {Thompson, William R.},
  year = {1933},
  journal = {Biometrika},
  volume = {25},
  number = {3/4},
  pages = {285--294},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2332286},
  file = {/Users/ludwig/Zotero/storage/EMPDF6VQ/Thompson - 1933 - On the Likelihood that One Unknown Probability Exc.pdf}
}

@article{tolovski2020AdvancingRenewableElectricity,
  title = {Advancing {{Renewable Electricity Consumption With Reinforcement Learning}}},
  author = {Tolovski, Filip},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.04310 [cs, eess, stat]},
  eprint = {2003.04310},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  abstract = {As the share of renewable energy sources in the present electric energy mix rises, their intermittence proves to be the biggest challenge to carbon free electricity generation. To address this challenge, we propose an electricity pricing agent, which sends price signals to the customers and contributes to shifting the customer demand to periods of high renewable energy generation. We propose an implementation of a pricing agent with a reinforcement learning approach where the environment is represented by the customers, the electricity generation utilities and the weather conditions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Electrical Engineering and Systems Science - Signal Processing,Electrical Engineering and Systems Science - Systems and Control,I.2.11,I.2.6,I.2.8,Statistics - Machine Learning},
  file = {/Users/ludwig/Zotero/storage/KCIZ3KZ5/Tolovski - 2020 - Advancing Renewable Electricity Consumption With R.pdf;/Users/ludwig/Zotero/storage/NHMYYUKL/2003.html}
}

@article{vazquez-canteli2019CityLearnV1OpenAI,
  title = {{{CityLearn}} v1.0: {{An OpenAI Gym Environment}} for {{Demand Response}} with {{Deep Reinforcement Learning}}},
  shorttitle = {{{CityLearn}} v1.0},
  author = {{V{\'a}zquez-Canteli}, Jos{\'e} and K{\"a}mpf, J{\'e}r{\^o}me and Henze, Gregor and Nagy, Zolt{\'a}n},
  year = {2019},
  journal = {Proceedings of the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
  publisher = {{ACM}},
  doi = {10.1145/3360322.3360998},
  abstract = {Demand response has the potential of reducing peaks of electricity demand by about 20\% in the US, where buildings represent roughly 70\% of the total electricity demand. Buildings are dynamic systems in constant change (i.e. occupants' behavior, refurbishment measures), which are costly to model and difficult to coordinate with other urban energy systems. Reinforcement learning is an adaptive control algorithm that can control these urban energy systems relying on historical and real-time data instead of models. Plenty of research has been conducted in the use of reinforcement learning for demand response applications in the last few years. However, most experiments are difficult to replicate, and the lack of standardization makes the performance of different algorithms difficult, if not impossible, to compare. In this demo, we introduce a new framework, CityLearn, based on the OpenAI Gym Environment, which will allow researchers to implement, share, replicate, and compare their implementations of reinforcement learning for demand response applications more easily. The framework is open source and modular, which allows researchers to modify and customize it, e.g., by adding additional storage, generation, or energy-consuming systems},
  file = {/Users/ludwig/Zotero/storage/3AN7E22K/V√°zquez-Canteli et al. - 2019 - CityLearn v1.0 An OpenAI Gym Environment for Dema.pdf}
}

@article{vazquez-canteli2019ReinforcementLearningDemand,
  title = {Reinforcement Learning for Demand Response: {{A}} Review of Algorithms and Modeling Techniques},
  shorttitle = {Reinforcement Learning for Demand Response},
  author = {{V{\'a}zquez-Canteli}, Jos{\'e} R. and Nagy, Zolt{\'a}n},
  year = {2019},
  month = feb,
  journal = {Applied Energy},
  volume = {235},
  pages = {1072--1089},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2018.11.002},
  abstract = {Buildings account for about 40\% of the global energy consumption. Renewable energy resources are one possibility to mitigate the dependence of residential buildings on the electrical grid. However, their integration into the existing grid infrastructure must be done carefully to avoid instability, and guarantee availability and security of supply. Demand response, or demand-side management, improves grid stability by increasing demand flexibility, and shifts peak demand towards periods of peak renewable energy generation by providing consumers with economic incentives. This paper reviews the use of reinforcement learning, a machine learning algorithm, for demand response applications in the smart grid. Reinforcement learning has been utilized to control diverse energy systems such as electric vehicles, heating ventilation and air conditioning (HVAC) systems, smart appliances, or batteries. The future of demand response greatly depends on its ability to prevent consumer discomfort and integrate human feedback into the control loop. Reinforcement learning is a potentially model-free algorithm that can adapt to its environment, as well as to human preferences by directly integrating user feedback into its control logic. Our review shows that, although many papers consider human comfort and satisfaction, most of them focus on single-agent systems with demand-independent electricity prices and a stationary environment. However, when electricity prices are modelled as demand-dependent variables, there is a risk of shifting the peak demand rather than shaving it. We identify a need to further explore reinforcement learning to coordinate multi-agent systems that can participate in demand response programs under demand-dependent electricity prices. Finally, we discuss directions for future research, e.g., quantifying how RL could adapt to changing urban conditions such as building refurbishment and urban or population growth.},
  langid = {english},
  keywords = {Building energy,Deep learning,Electric vehicles,HVAC control,Machine learning,Smart grid},
  file = {/Users/ludwig/Zotero/storage/RT7VKS6Q/V√°zquez-Canteli und Nagy - 2019 - Reinforcement learning for demand response A revi.pdf;/Users/ludwig/Zotero/storage/YMJEYZ3I/S0306261918317082.html}
}

@inproceedings{vazquez-canteli2020CityLearnChallenge2020,
  title = {The {{CityLearn Challenge}} 2020},
  booktitle = {Proceedings of the 7th {{ACM International Conference}} on {{Systems}} for {{Energy-Efficient Buildings}}, {{Cities}}, and {{Transportation}}},
  author = {{V{\'a}zquez-Canteli}, Jos{\'e} R. and Dey, Sourav and Henze, Gregor and Nagy, Zoltan},
  year = {2020},
  month = nov,
  series = {{{BuildSys}} '20},
  pages = {320--321},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3408308.3431122},
  abstract = {This poster shows the results of The CityLearn Challenge 2020. 4 teams competed over 6 months to design the best reinforcement learning agent for the energy management of a microgrid.},
  isbn = {978-1-4503-8061-4},
  keywords = {demand response,microgrid,Reinforcement learning},
  file = {/Users/ludwig/Zotero/storage/JYW3HZV9/V√°zquez-Canteli et al. - 2020 - The CityLearn Challenge 2020.pdf}
}

@article{vazquez-canteli2020CityLearnStandardizingResearch,
  title = {{{CityLearn}}: {{Standardizing Research}} in {{Multi-Agent Reinforcement Learning}} for {{Demand Response}} and {{Urban Energy Management}}},
  shorttitle = {{{CityLearn}}},
  author = {{Vazquez-Canteli}, Jose R. and Dey, Sourav and Henze, Gregor and Nagy, Zoltan},
  year = {2020},
  month = dec,
  journal = {arXiv:2012.10504 [cs]},
  eprint = {2012.10504},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Rapid urbanization, increasing integration of distributed renewable energy resources, energy storage, and electric vehicles introduce new challenges for the power grid. In the US, buildings represent about 70\% of the total electricity demand and demand response has the potential for reducing peaks of electricity by about 20\%. Unlocking this potential requires control systems that operate on distributed systems, ideally data-driven and model-free. For this, reinforcement learning (RL) algorithms have gained increased interest in the past years. However, research in RL for demand response has been lacking the level of standardization that propelled the enormous progress in RL research in the computer science community. To remedy this, we created CityLearn, an OpenAI Gym Environment which allows researchers to implement, share, replicate, and compare their implementations of RL for demand response. Here, we discuss this environment and The CityLearn Challenge, a RL competition we organized to propel further progress in this field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.1},
  file = {/Users/ludwig/Zotero/storage/DBJI4FX7/Vazquez-Canteli et al. - 2020 - CityLearn Standardizing Research in Multi-Agent R.pdf;/Users/ludwig/Zotero/storage/83VQ93AD/2012.html}
}

@inproceedings{vazquez-canteli2020MARLISAMultiAgentReinforcement,
  title = {{{MARLISA}}: {{Multi-Agent Reinforcement Learning}} with {{Iterative Sequential Action Selection}} for {{Load Shaping}} of {{Grid-Interactive Connected Buildings}}},
  shorttitle = {{{MARLISA}}},
  booktitle = {Proceedings of the 7th {{ACM International Conference}} on {{Systems}} for {{Energy-Efficient Buildings}}, {{Cities}}, and {{Transportation}}},
  author = {{Vazquez-Canteli}, Jose R. and Henze, Gregor and Nagy, Zoltan},
  year = {2020},
  month = nov,
  series = {{{BuildSys}} '20},
  pages = {170--179},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3408308.3427604},
  abstract = {We demonstrate that multi-agent reinforcement learning (RL) controllers can cooperate to provide more effective load shaping in a model-free, decentralized, and scalable way with very limited sharing of anonymous information. Rapid urbanization, increasing electrification, the integration of renewable energy resources, and the potential shift towards electric vehicles create new challenges for the planning and control of energy systems in smart cities. Energy storage resources can help better align peaks of renewable energy generation with peaks of electricity consumption and flatten the curve of electricity demand. Model-based controllers, such as MPC, require developing models of the systems controlled, which is often not cost-effective or scalable. Model-free controllers, such as RL, have the potential to provide good control policies cost-effectively and leverage the use of historical data for training. However, it is unclear how RL algorithms can control a multitude of energy systems in a scalable coordinated way. In this paper, we introduce MARLISA, a controller that combines multi-agent RL with our proposed iterative sequential action selection algorithm for load shaping in urban energy systems. This approach uses a reward function with individual and collective goals, and the agents predict their own future electricity consumption and share this information with each other following a leader-follower schema. The RL agents are tested in four groups of nine simulated buildings, with each group located in a different climate. The buildings have diverse load and domestic hot water profiles, PV panels, thermal storage devices, heat pumps, and electric heaters. The agents are evaluated on the average of five normalized metrics: annual net electric consumption, 1 -- load factor, average daily peak demand, annual peak demand, and ramping. MARLISA achieves superior results over multiple independent/uncooperative RL agents using the same reward function. Our results outperformed a manually optimized rule-based controller (RBC) benchmark by reducing the average daily peak load by 15\%, ramping by 35\%, and increasing the load factor by 10\%. A multi-year case study on real weather data shows that MARLISA significantly outperforms the RBC in within a year and converges in less than 2 years. Combining MARLISA and the RBC for the first year improves overall initial performance by learning from the RBC rather than random exploration.},
  isbn = {978-1-4503-8061-4},
  keywords = {demand response,microgrid,multi-agent coordination,Reinforcement learning},
  file = {/Users/ludwig/Zotero/storage/JWQAXBAQ/Vazquez-Canteli et al. - 2020 - MARLISA Multi-Agent Reinforcement Learning with I.pdf}
}

@article{wang2020ReinforcementLearningBuilding,
  title = {Reinforcement Learning for Building Controls: {{The}} Opportunities and Challenges},
  shorttitle = {Reinforcement Learning for Building Controls},
  author = {Wang, Zhe and Hong, Tianzhen},
  year = {2020},
  month = jul,
  journal = {Applied Energy},
  volume = {269},
  pages = {115036},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2020.115036},
  abstract = {Building controls are becoming more important and complicated due to the dynamic and stochastic energy demand, on-site intermittent energy supply, as well as energy storage, making it difficult for them to be optimized by conventional control techniques. Reinforcement Learning (RL), as an emerging control technique, has attracted growing research interest and demonstrated its potential to enhance building performance while addressing some limitations of other advanced control techniques, such as model predictive control. This study conducted a comprehensive review of existing studies that applied RL for building controls. It provided a detailed breakdown of the existing RL studies that use a specific variation of each major component of the Reinforcement Learning: algorithm, state, action, reward, and environment. We found RL for building controls is still in the research stage with limited applications (11\%) in real buildings. Three significant barriers prevent the adoption of RL controllers in actual building controls: (1) the training process is time consuming and data demanding, (2) the control security and robustness need to be enhanced, and (3) the generalization capabilities of RL controllers need to be improved using approaches such as transfer learning. Future research may focus on developing RL controllers that could be used in real buildings, addressing current RL challenges, such as accelerating training and enhancing control robustness, as well as developing an open-source testbed and dataset for performance benchmarking of RL controllers.},
  langid = {english},
  keywords = {Building controls,Building performance,Machine learning,Optimization,Reinforcement learning},
  file = {/Users/ludwig/Zotero/storage/KPTMM4PJ/Wang und Hong - 2020 - Reinforcement learning for building controls The .pdf;/Users/ludwig/Zotero/storage/X5Z6XLY5/S0306261920305481.html}
}

@techreport{wang2022UncertaintyDecisionSurvey,
  type = {Preprint},
  title = {Towards {{Uncertainty}} in {{Decision}}: {{A Survey}} on {{Recent Advances}} and {{Challenges}} in {{Bayesian Reinforcement Learning}}},
  shorttitle = {Towards {{Uncertainty}} in {{Decision}}},
  author = {Wang, Zicheng and Meng, Hua and Zhou, Zhengchun and Feng, Yanghe and Gao, Yang and Yu, Chao},
  year = {2022},
  month = jun,
  institution = {{In Review}},
  doi = {10.21203/rs.3.rs-1780336/v1},
  abstract = {Abstract           Reinforcement learning is a research paradigm that is commonly utilized to tackle problems involving sequential decision-making. Agents learn optimum policy from samples generated by interacting with uncertain environments. Bayesian reinforcement learning presents the uncertainty measure of probability significance based on sequential decision-making, which plays a proper auxiliary function in the optimization of agent control policies, attracting a lot of study attention. This paper provides an overview of contemporary Bayesian and reinforcement learning methods in this setting. We begin by outlining the many types of reinforcement learning and Bayesian approaches, as well as their characteristics. Then we review the most sophisticated Bayesian reinforcement learning approaches from the perspective of uncertainty measurement and control, as well as summarize and discuss various challenges and related research in its expansion to complicated control tasks. Applying the Bayesian technique with reinforcement learning in diverse disciplines are then discussed. Finally, we highlight the current challenges of Bayesian reinforcement learning and future research topics that might help promote Bayesian reinforcement learning.},
  langid = {english},
  file = {/Users/ludwig/Zotero/storage/83ABJ84I/Wang et al. - 2022 - Towards Uncertainty in Decision A Survey on Recen.pdf}
}

@article{yang2020ReinforcementLearningSustainable,
  title = {Reinforcement Learning in Sustainable Energy and Electric Systems: A Survey},
  shorttitle = {Reinforcement Learning in Sustainable Energy and Electric Systems},
  author = {Yang, Ting and Zhao, Liyuan and Li, Wei and Zomaya, Albert Y.},
  year = {2020},
  month = jan,
  journal = {Annual Reviews in Control},
  volume = {49},
  pages = {145--163},
  issn = {1367-5788},
  doi = {10.1016/j.arcontrol.2020.03.001},
  abstract = {The dynamic nature of sustainable energy and electric systems can vary significantly along with the environment and load change, and they represent the features of multivariate, high complexity and uncertainty of the nonlinear system. Moreover, the integration of intermittent renewable energy sources and energy consumption behaviours of households introduce more uncertainty into sustainable energy and electric systems. The operation, control and decision-making in such an environment definitely require increasing intelligence and flexibility in the control and optimization to ensure the quality of service of sustainable energy and electric systems. Reinforcement learning is a wide class of optimal control strategies that uses estimating value functions from experience, simulation, or search to learn in highly dynamic, stochastic environment. The interactive context enables reinforcement learning to develop strong learning ability and high adaptability. Reinforcement learning does not require the use of the model of system dynamics, which makes it suitable for sustainable energy and electric systems with complex nonlinearity and uncertainty. The use of reinforcement learning in sustainable energy and electric systems will certainly change the traditional energy utilization mode and bring more intelligence into the system. In this survey, an overview of reinforcement learning, the demand for reinforcement learning in sustainable energy and electric systems, reinforcement learning applications in sustainable energy and electric systems, and future challenges and opportunities will be explicitly addressed.},
  langid = {english},
  keywords = {Deep reinforcement learning,Integrated energy system,Power system,Reinforcement learning,Sustainable energy and electric systems},
  file = {/Users/ludwig/Zotero/storage/XIX8TQKD/Yang et al. - 2020 - Reinforcement learning in sustainable energy and e.pdf;/Users/ludwig/Zotero/storage/S6CTFR2G/S1367578820300079.html}
}
