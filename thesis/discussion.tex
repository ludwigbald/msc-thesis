%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Diskussion und Ausblick
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Discussion}
  \label{Discussion}

Through the experiments described in the previous chapters, I evaluated the Uncertainty-Aware Deep Q Network algorithm on a custom building-scale energy-management environment based on the 2022 CityLearn challenge.
Compared to two versions of the simpler DQN algorithm, tuned UA-DQN requires fewer interactions with the environment to arrive at a good performance.
However, it does not reliably exceed the performance of a baseline rule-based control policy.
In this chapter, I discuss the collected evidence and limitations of the experiment and point out several possible ways of further improving the algorithm's performance.

\section{Preparation}
% Rule-Based Agent
% Summarize what I did
The baseline rule-based policy was manually derived from simple patterns observed in the data.
Its performance is compared to the performance of an idle policy, which does not make use the battery at all.
It enables substantial savings in electricity cost and carbon emissions.
% Interpret results
Compared to the idle policy, more of the generated solar electricity is used.
Therefore, overall, less electricity is purchased from the grid.
The policy also shifted the time of electricity purchase to an earlier time of day.
Especially during peak hours, when both price and carbon intensity are high, the policy achieves a substantial reduction in building electricity demand.
The policy does dynamically adapt to observed solar generation and electricity use, but it can not predict the amount of excess solar production to be stored, instead simply assuming it does not change from the hour before.
This is an obvious limitation of the policy.

Overall, the rule-based policy can serve as a useful baseline for evaluating the performance of more complex algorithms.
It is a simple algorithm that captures a lot of the storage potential of the system, but is not finely optimized on minute details.

% First Experiment: Discretization
% Summarize what I did
In order to determine the simplest discrete action space for the DQN variants that would allow them to perform well, I perform a pre-experiment on the rule-based policy.
The experiment tests the performance of the rule-based policy on different resolutions of discretized action space against its performance on the continuous action space.

% Discuss findings
Figure \ref{fig:discretization} shows a large dependency of the rule-based policy on resolution of the action space.
As expected, a higher discretization resolution approaches the continuous case and leads to better performance.
However, the policy's performance is particularly impacted because it relies on a fixed number as minimum charging action of 0.24 in the hours before the high-price period starts.
This number is discretized into more or less useful values that for example don't fully charge the battery.


\section{RL Algorithms}
% Second Experiment: Tuning
% Summarize what I did
After establishing the baseline and environment details, I identify and tune important hyperparameters of UA-DQN and two versions of DQN for best performance.
I then rerun the tuned algorithms with multiple random seeds to get an estimate of their reliability.

% sensitivity to hyperparameters
The results of the tuning runs, presented in figure \ref{fig:tuning_results}, show that the performance of all algorithms depends on hyperparameter choice.
Comparing the algorithms, UA-DQN performs better for a wider range of hyperparameter choices.
For each algorithm, the most important hyperparameters are reported in \ref{tab:hyperparameters}.
Across all algorithms, the learning rate is the most important hyperparameter as measured by correlation with the final score.
On average, a smaller learning rate leads to a better performance.
This is mainly due to the fact that a high learning rate destabilizes the algorithm.
Adam's $\epsilon$ did not have a large effect on either algorithm.

% general robustness
Some tuning runs failed. All UA-DQN runs with a batch size of 1 failed because the quantile Huber loss function needs a batch size of at least 2, but for correct hyperparameters the algorithm proved robust.
The DQN-Softmax algorithm failed to converge on some runs with a batch size of 8. Even the tuned variant only completed 9/10 runs. This suggests some inherent instability of the algorithm.


% action selection
Figure \ref{fig:non_greedy_fraction} shows the fraction of non-greedy actions taken by the tuned algorithms.
A non-greedy action is an action that was taken in order to explore the environment rather than exploit current knowledge.
UA-DQN explores more throughout the whole run than the other strategies.
$\epsilon$-greedy DQN explores much less than the other strategies.
A more sophisticated and tuned schedule for the exploration rate $\epsilon$ could improve the algorithm's performance at the added cost of requiring more tuning.

% general performance
Figure \ref{fig:tuning_validation} shows that UA-DQN improves its performance much faster than the other algorithms.
Especially in the first 5 episodes, when UA-DQN explores less than DQN-Softmax, UA-DQN performance improves much faster.
The reason for this could be either that tuned UA-DQN takes more useful exploratory actions, or that it integrates observed information more effectively, or a combination of both.

% Limitations of the experiment:
% Problems caused by tuning setup
The used tuning method favors risky hyperparameter choices.
Because every combination of hyperparameters is tried only once, accepting the single best tuning run probably means accepting an outlier performance for this combination of hyperparameters.
For example, the best performing runs of UA-DQN and DQN-Softmax share a low target network update frequency, which causes both faster learning and instability. Similarly, the best performing run of $\epsilon$-greedy DQN uses a small batch size of 4.
Therefore, the hyperparameters selected by the tuning process are biased towards being less reliable.
This effect might have further helped UA-DQN, which was tuned on half as many runs as the other algorithms, so there was less opportunity for risky hyperparameters to outperform more robust choices.
However, even with fewer tuning runs, the best UA-DQN tuning run significantly outperformed the DQN variants.
A different tuning procedure might have favored more reliable algorithms with a better mean performance.
One further possible source of bias is the networks' initialization, which was the same for all algorithms during tuning.

% Short summary
In summary, the experiment shows the potential of UA-DQN, which is able to take more useful exploratory actions.
The experiment suffers from multiple potential sources of bias, and a tuning procedure that favors unstable hyperparameters.
UA-DQN approaches the performance of the rule-based baseline and learns from fewer environment interactions than simpler DQN variants, but has much larger resource demands (see table \ref{tab:resources}).


\section{General Discussion}
% Limitations of the custom environment (external validity)
The custom environment based on the 2022 CityLearn challenge presents a planning and control task based on real-world data.
Some of the experiment's findings can be expected to generalize to real-world applications of the algorithms.
Particularly, the environment requires the agent to learn patterns in electricity price and carbon intensity, and domestic electricity usage and production.
The strategy task of when to use electricity storage is modelled well by CityLearn.
It depends on long-term, aggregate data and patterns on the scale of multiple hours.
In contrast, a part of the rule-based policy's aim is to try to store precisely the amount of excess electricity generated. This is nearly impossible to do in CityLearn, but easy to implement in a real-world building control system.
This limits the effectiveness of a good long-term plan.

The environment used in the experiments only provided a subset of CityLearn's data as observed features.
For example, features included only one weather forecast variable, no information on day of the week or month, and no price forecasts.
Real-world weather forecasts are uncertain, which requires a different strategy for acting under uncertainty.
In a real-world application, a smart home system might also have access to occupant behavioral data, like knowing if someone is home or not, which can lead to significantly better plans.

While the building data stems from only one building in a particular climate zone, the algorithms' effectiveness depends mostly on the daily patterns of pricing.
More complex or dynamic pricing models might require more sophisticated planning algorithms.
In fact, coordination strategies such as dynamic pricing are another goal not addressed by this environment.
A real-world system could likely not only buy, but also sell electricity to and from the grid, providing flexibility not only to the building.

% CityLearn in general
The CityLearn framework is useful for developing experiments that test the application of RL planning algorithms to building energy control tasks.
It's an efficient environment that models important parts of a building and can incorporate real-world data.
However, in the current version, it does not serve as a realistic model for the full task.
Applying RL for building energy control is an area of ongoing research, and \cite{nweye2022RealworldChallengesMultiagent} name a number of challenges to be overcome when applying RL to grid-integrated buildings.

% General discussion: Can we answer the research questions?
The experiment described in this thesis shows that UA-DQN can explore a simple environment more efficiently than other RL algorithms.
This provides further evidence for the usefulness of uncertainty-aware RL methods when acting in high-stakes environments.
However, in this experiment, UA-DQN did not outperform the rule-based policy.
A real-life building energy demand response task might be more complex, which might favor a machine learning approach.

% Further research: See Conclusion