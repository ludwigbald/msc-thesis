%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Zusammenfassung
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusion and Outlook}
  \label{conclusion}

\todo[inline]{
- 1 page
- summarize again what your paper did, but now emphasize more the results, and comparisons
- write conclusions that can be drawn from the results found and the discussion presented in the paper
- future work (be very brief, explain what, but not much how)


Summary:
- I evaluate UA-DQN on a custom task using CityLearn.
- Tuned UA-DQN requires fewer environment interactions than variants of DQN to learn a good policy. This suggests that efficient exploration is beneficial, and exploration in general is useful for Q-Learning on CityLearn.
- Tuned UA-DQN does not outperform a simple rule-based policy.

Conclusions:
- UA-DQN is a promising algorithm for environments where exploration is costly.

Future Work:
- CityLearn with integrated control
- Coordination task
- Uncertain Weather forecasts.
- how does UA-DQN with a nonstationary environment? How much does a learned policy transfer to a different building, for example?
}

In this thesis, I apply the UA-DQN algorithm to a custom building energy management task implemented in CityLearn.
I find that tuned UA-DQN outperforms DQN in order to learn a good policy. This means things are harder to get right.



